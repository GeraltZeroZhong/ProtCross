import os
import torch
import numpy as np
import pandas as pd
from sklearn.metrics import roc_curve, auc, average_precision_score, f1_score
from torch_geometric.loader import DataLoader
import sys
import glob

# === ğŸ› ï¸ é…ç½®åŒºåŸŸ (æ ¹æ®ä½ çš„å®é™…æ–‡ä»¶è·¯å¾„ä¿®æ”¹) ===
# æ ¼å¼: {ç§å­: "Checkpointæ–‡ä»¶è·¯å¾„"}
CKPT_MAP = {
    1:   "checkpoints/last.ckpt",
    2025:   "checkpoints/last-v1.ckpt",      # ç¬¬1æ¬¡è·‘çš„
    1224: "checkpoints/last-v2.ckpt",   # ç¬¬2æ¬¡è·‘çš„
    318: "checkpoints/last-v3.ckpt"    # ç¬¬3æ¬¡è·‘çš„
}

AF2_DATA_FOLDER = "data/processed_af2"
OUTPUT_CSV = "experiment_results_summary.csv"

# å¼•å…¥é¡¹ç›®æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from src.evopoint_da.models.module import EvoPointDALitModule
from scripts.plot_metrics import SimpleFolderDataset 

def evaluate_model(ckpt_path, data_folder, seed):
    """åŠ è½½æ¨¡å‹å¹¶è¯„ä¼°æ ¸å¿ƒæŒ‡æ ‡"""
    print(f"\n{'='*40}")
    print(f"ğŸ“Š Evaluating SEED {seed}")
    print(f"ğŸ“‚ Checkpoint: {ckpt_path}")
    print(f"{'='*40}")
    
    if not os.path.exists(ckpt_path):
        print(f"âŒ Error: File not found: {ckpt_path}")
        return None

    # åŠ è½½æ¨¡å‹
    try:
        model = EvoPointDALitModule.load_from_checkpoint(ckpt_path)
        model.eval()
        model.cuda()
    except Exception as e:
        print(f"âŒ Error loading checkpoint: {e}")
        return None
    
    # åŠ è½½æ•°æ®
    dataset = SimpleFolderDataset(data_folder)
    loader = DataLoader(dataset, batch_size=1, shuffle=False)
    
    all_labels = []
    all_probs = []
    
    print("Running Inference...")
    with torch.no_grad():
        for i, batch in enumerate(loader):
            batch = batch.cuda()
            
            # Forward Pass
            src_x = batch.x if model.hparams.use_esm else None
            feats, _ = model.backbone(src_x, batch.pos, batch.batch)
            logits = model.seg_head(feats)
            probs = torch.softmax(logits, dim=1)[:, 1]
            
            # Confidence Gating
            p = model._normalize_plddt(batch.plddt).squeeze()
            is_reliable = (p > 0.65).float()
            probs_gated = probs * is_reliable
            
            all_labels.append(batch.y.cpu().numpy())
            all_probs.append(probs_gated.cpu().numpy())
            
            if i % 50 == 0:
                print(f"Processing {i}/{len(loader)}...")

    # è®¡ç®—æŒ‡æ ‡
    y_true = np.concatenate(all_labels)
    y_scores = np.concatenate(all_probs)
    
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc_score = auc(fpr, tpr)
    ap_score = average_precision_score(y_true, y_scores)
    
    # å¯»æ‰¾æœ€ä½³ F1
    best_f1 = 0
    best_thresh = 0
    for thresh in np.arange(0.1, 1.0, 0.05):
        y_pred = (y_scores > thresh).astype(int)
        score = f1_score(y_true, y_pred)
        if score > best_f1:
            best_f1 = score
            best_thresh = thresh
            
    print(f"âœ… Result: AUC={auc_score:.4f}, AP={ap_score:.4f}, F1={best_f1:.4f}")
    
    return {
        "Seed": seed,
        "AUC": auc_score,
        "AP": ap_score,
        "F1_Max": best_f1,
        "Best_Threshold": best_thresh,
        "Checkpoint": ckpt_path
    }

def main():
    results = []
    
    # éå†é¢„å®šä¹‰çš„ Checkpoint åˆ—è¡¨
    for seed, ckpt_path in CKPT_MAP.items():
        metrics = evaluate_model(ckpt_path, AF2_DATA_FOLDER, seed)
        
        if metrics:
            results.append(metrics)
            # å®æ—¶ä¿å­˜
            df_current = pd.DataFrame(results)
            df_current.to_csv(OUTPUT_CSV, index=False)

    # === æœ€ç»ˆæ±‡æ€» ===
    if len(results) > 0:
        df = pd.DataFrame(results)
        print("\n" + "="*50)
        print("ğŸ† FINAL SUMMARY (Mean Â± Std)")
        print("="*50)
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        mean_auc = df["AUC"].mean()
        std_auc = df["AUC"].std()
        print(f"AUC : {mean_auc:.4f} Â± {std_auc:.4f}")
        
        mean_ap = df["AP"].mean()
        std_ap = df["AP"].std()
        print(f"AP  : {mean_ap:.4f} Â± {std_ap:.4f}")
        
        mean_f1 = df["F1_Max"].mean()
        std_f1 = df["F1_Max"].std()
        print(f"F1  : {mean_f1:.4f} Â± {std_f1:.4f}")
        print("="*50)
        
        # æ·»åŠ æ±‡æ€»è¡Œ
        summary_row = {
            "Seed": "Mean Â± Std",
            "AUC": f"{mean_auc:.4f} Â± {std_auc:.4f}",
            "AP": f"{mean_ap:.4f} Â± {std_ap:.4f}",
            "F1_Max": f"{mean_f1:.4f} Â± {std_f1:.4f}",
            "Best_Threshold": "-",
            "Checkpoint": "-"
        }
        
        df_final = pd.concat([df, pd.DataFrame([summary_row])], ignore_index=True)
        df_final.to_csv(OUTPUT_CSV, index=False)
        print(f"ğŸ“ Full report saved to: {OUTPUT_CSV}")
    else:
        print("âŒ No results obtained. Please check file paths.")

if __name__ == "__main__":
    main()
