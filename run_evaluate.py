import os
import torch
import numpy as np
import pandas as pd
from sklearn.metrics import roc_curve, auc, average_precision_score, f1_score
from torch_geometric.loader import DataLoader
import sys
import glob

# === ğŸ› ï¸ é…ç½®åŒºåŸŸ ===
WEIGHTS_DIR = "saved_weights"       # è‡ªåŠ¨æ‰«æçš„æ ¹ç›®å½•
AF2_DATA_FOLDER = "data/processed_af2"
OUTPUT_CSV = "evaluation_results_all.csv"

# å¼•å…¥é¡¹ç›®æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from evopoint_da.models.module import EvoPointDALitModule
# [ä¿®æ”¹ 1] å¼•å…¥ EvoPointDataset
from evopoint_da.data.dataset import EvoPointDataset

def get_checkpoints_from_dir(root_dir):
    """
    æ‰«æ saved_weights ç›®å½•ï¼Œè§£æå­æ–‡ä»¶å¤¹ç»“æ„
    è¿”å›åˆ—è¡¨: [{'path': ..., 'exp_id': ..., 'seed': ...}, ...]
    """
    if not os.path.exists(root_dir):
        print(f"âŒ Error: Directory '{root_dir}' not found.")
        return []

    ckpt_list = []
    
    # éå† root_dir ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹
    subdirs = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])
    
    print(f"ğŸ“‚ Found {len(subdirs)} experiment folders in '{root_dir}':")
    
    for folder_name in subdirs:
        folder_path = os.path.join(root_dir, folder_name)
        
        # 1. å°è¯•å¯»æ‰¾ last.ckpt
        target_ckpt = os.path.join(folder_path, "last.ckpt")
        if not os.path.exists(target_ckpt):
            # å¦‚æœ last.ckpt ä¸å­˜åœ¨ï¼Œå°è¯•æ‰¾ä»»æ„ .ckpt
            candidates = glob.glob(os.path.join(folder_path, "*.ckpt"))
            if candidates:
                target_ckpt = candidates[0] # å–ç¬¬ä¸€ä¸ª
                print(f"   âš ï¸ 'last.ckpt' missing in {folder_name}, using fallback: {os.path.basename(target_ckpt)}")
            else:
                print(f"   âŒ No .ckpt files found in {folder_name}, skipping.")
                continue
        
        # 2. è§£æå…ƒæ•°æ® (Exp_Seed)
        # å‡è®¾æ–‡ä»¶å¤¹å‘½åæ ¼å¼ä¸º "ExperimentID_Seed" (ä¾‹å¦‚ "A_42")
        parts = folder_name.split('_')
        if len(parts) >= 2 and parts[-1].isdigit():
            seed = int(parts[-1])
            exp_id = "_".join(parts[:-1]) # å¤„ç†åå­—ä¸­æœ‰ä¸‹åˆ’çº¿çš„æƒ…å†µ
        else:
            seed = "Unknown"
            exp_id = folder_name
            
        ckpt_list.append({
            "path": target_ckpt,
            "exp_id": exp_id,
            "seed": seed,
            "folder": folder_name
        })
        print(f"   âœ… Found: {exp_id} (Seed {seed}) -> {os.path.basename(target_ckpt)}")

    return ckpt_list

def evaluate_model(ckpt_info, data_folder):
    """åŠ è½½æ¨¡å‹å¹¶è¯„ä¼°æ ¸å¿ƒæŒ‡æ ‡"""
    ckpt_path = ckpt_info['path']
    seed = ckpt_info['seed']
    exp_id = ckpt_info['exp_id']
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Evaluating Exp: {exp_id} | Seed: {seed}")
    print(f"ğŸ“‚ Load: {ckpt_path}")
    print(f"{'='*60}")
    
    # åŠ è½½æ¨¡å‹
    try:
        model = EvoPointDALitModule.load_from_checkpoint(ckpt_path)
        model.eval()
        if torch.cuda.is_available():
            model.cuda()
    except Exception as e:
        print(f"âŒ Error loading checkpoint: {e}")
        return None
    
    # åŠ è½½æ•°æ® [ä¿®æ”¹ 2] ä½¿ç”¨ EvoPointDataset
    try:
        dataset = EvoPointDataset(root=data_folder, split="test")
        print(f"   âœ… Loaded EvoPointDataset (split='test', n={len(dataset)})")
    except Exception as e:
        print(f"   âš ï¸ 'test' split failed, trying 'train' split ({e})")
        dataset = EvoPointDataset(root=data_folder, split="train")
        print(f"   âœ… Loaded EvoPointDataset (split='train', n={len(dataset)})")

    loader = DataLoader(dataset, batch_size=1, shuffle=False)
    
    all_labels = []
    all_probs = []
    
    print("   Running Inference...")
    with torch.no_grad():
        for i, batch in enumerate(loader):
            if torch.cuda.is_available():
                batch = batch.cuda()
            
            # Forward Pass
            src_x = batch.x if model.hparams.use_esm else None
            feats, _ = model.backbone(src_x, batch.pos, batch.batch)
            logits = model.seg_head(feats)
            probs = torch.softmax(logits, dim=1)[:, 1]
            
            # [æ³¨æ„] æ­¤å¤„ä¿æŒé€»è¾‘ä¸€è‡´ï¼Œä¸å¼€å¯ç¡¬é˜ˆå€¼è¿‡æ»¤ (Raw Probabilities)
            # å¦‚æœéœ€è¦å¼€å¯ï¼Œè¯·å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç ï¼š
            # if hasattr(model, '_normalize_plddt') and getattr(model.hparams, 'use_plddt_weight', False):
            #     p = model._normalize_plddt(batch.plddt).squeeze()
            #     is_reliable = (p > 0.65).float()
            #     probs = probs * is_reliable
            
            all_labels.append(batch.y.cpu().numpy())
            all_probs.append(probs.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    y_true = np.concatenate(all_labels)
    y_scores = np.concatenate(all_probs)
    
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc_score = auc(fpr, tpr)
    ap_score = average_precision_score(y_true, y_scores)
    
    # å¯»æ‰¾æœ€ä½³ F1
    best_f1 = 0
    best_thresh = 0
    for thresh in np.arange(0.1, 1.0, 0.05):
        y_pred = (y_scores > thresh).astype(int)
        score = f1_score(y_true, y_pred)
        if score > best_f1:
            best_f1 = score
            best_thresh = thresh
            
    print(f"   âœ… Result: AUC={auc_score:.4f}, AP={ap_score:.4f}, F1={best_f1:.4f}")
    
    return {
        "Experiment": exp_id,
        "Seed": seed,
        "AUC": auc_score,
        "AP": ap_score,
        "F1_Max": best_f1,
        "Best_Threshold": best_thresh,
        "Checkpoint": ckpt_path
    }

def main():
    # 1. æ‰«ææ–‡ä»¶å¤¹
    tasks = get_checkpoints_from_dir(WEIGHTS_DIR)
    
    if not tasks:
        print("No checkpoints found to evaluate.")
        return

    results = []
    
    # 2. ä¾æ¬¡è¯„ä¼°
    for task in tasks:
        metrics = evaluate_model(task, AF2_DATA_FOLDER)
        if metrics:
            results.append(metrics)
            # å®æ—¶ä¿å­˜ï¼Œé˜²æ­¢ä¸­æ–­
            df_current = pd.DataFrame(results)
            df_current.to_csv(OUTPUT_CSV, index=False)

    # 3. æœ€ç»ˆæ±‡æ€»
    if len(results) > 0:
        df = pd.DataFrame(results)
        
        # æŒ‰ Experiment åˆ†ç»„è®¡ç®—å‡å€¼
        print("\n" + "="*80)
        print("ğŸ† GROUPED SUMMARY (Mean Â± Std)")
        print("="*80)
        
        # å°è¯•æŒ‰ Experiment åˆ†ç»„ç»Ÿè®¡ï¼Œå¦‚æœ Experiment éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå°±æ•´ä½“ç»Ÿè®¡
        if "Experiment" in df.columns:
            groups = df.groupby("Experiment")
            summary_rows = []
            
            print(f"{'Experiment':<25} | {'AUC':<20} | {'AP':<20}")
            print("-" * 75)
            
            for name, group in groups:
                m_auc, s_auc = group["AUC"].mean(), group["AUC"].std()
                m_ap, s_ap = group["AP"].mean(), group["AP"].std()
                
                # å¤„ç†åªæœ‰ä¸€ä¸ªæ ·æœ¬ std ä¸º NaN çš„æƒ…å†µ
                s_auc = 0.0 if np.isnan(s_auc) else s_auc
                s_ap = 0.0 if np.isnan(s_ap) else s_ap
                
                auc_str = f"{m_auc:.4f} Â± {s_auc:.4f}"
                ap_str = f"{m_ap:.4f} Â± {s_ap:.4f}"
                
                print(f"{name:<25} | {auc_str:<20} | {ap_str:<20}")
                
                summary_rows.append({
                    "Experiment": f"{name} (Mean)",
                    "Seed": "Aggregated",
                    "AUC": auc_str,
                    "AP": ap_str,
                    "F1_Max": f"{group['F1_Max'].mean():.4f}",
                    "Best_Threshold": "-",
                    "Checkpoint": "-"
                })
            
            # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ° CSV åº•éƒ¨
            df_final = pd.concat([df, pd.DataFrame(summary_rows)], ignore_index=True)
            df_final.to_csv(OUTPUT_CSV, index=False)
            
        else:
            # æ—§é€»è¾‘ï¼šæ•´ä½“ç»Ÿè®¡
            print(f"AUC : {df['AUC'].mean():.4f} Â± {df['AUC'].std():.4f}")

        print(f"\nğŸ“ Full report saved to: {OUTPUT_CSV}")
    else:
        print("âŒ No results obtained.")

if __name__ == "__main__":
    main()
