import os
import torch
import numpy as np
import pandas as pd
from sklearn.metrics import jaccard_score
from torch_geometric.loader import DataLoader
import sys
import glob
from tqdm import tqdm
import pytorch_lightning as pl  # 1. å¼•å…¥ PyTorch Lightning

# === ğŸ› ï¸ é…ç½®åŒºåŸŸ ===
WEIGHTS_DIR = "saved_weights"       # æƒé‡å¤‡ä»½ç›®å½•
AF2_DATA_FOLDER = "data/processed_af2"
OUTPUT_CSV = "standard_iou_results.csv"  # ç»“æœä¿å­˜æ–‡ä»¶
SEED = 42 # å›ºå®šç§å­

# å¼•å…¥é¡¹ç›®æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from src.evopoint_da.models.module import EvoPointDALitModule
from scripts.plot_metrics import SimpleFolderDataset 

def get_checkpoints_from_dir(root_dir):
    """è‡ªåŠ¨æ‰«æ saved_weights ç›®å½•"""
    if not os.path.exists(root_dir):
        print(f"âŒ Error: Directory '{root_dir}' not found.")
        return []

    ckpt_list = []
    subdirs = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])
    
    print(f"ğŸ“‚ Found {len(subdirs)} experiment folders in '{root_dir}':")
    
    for folder_name in subdirs:
        folder_path = os.path.join(root_dir, folder_name)
        
        # ä¼˜å…ˆæ‰¾ last.ckpt
        target_ckpt = os.path.join(folder_path, "last.ckpt")
        if not os.path.exists(target_ckpt):
            candidates = glob.glob(os.path.join(folder_path, "*.ckpt"))
            if candidates:
                target_ckpt = candidates[0]
            else:
                continue
        
        # è§£æå…ƒæ•°æ®
        parts = folder_name.split('_')
        if len(parts) >= 2 and parts[-1].isdigit():
            seed = int(parts[-1])
            exp_id = "_".join(parts[:-1])
        else:
            seed = "Unknown"
            exp_id = folder_name
            
        ckpt_list.append({"path": target_ckpt, "exp_id": exp_id, "seed": seed})

    return ckpt_list

def evaluate_standard_iou(ckpt_info, data_folder):
    """ä»…è®¡ç®—æœ€ä½³ Standard IoU (Binding Site)"""
    ckpt_path = ckpt_info['path']
    seed = ckpt_info['seed']
    exp_id = ckpt_info['exp_id']
    
    print(f"\nğŸ“Š Processing {exp_id} | Seed {seed}...")
    
    # 1. åŠ è½½æ¨¡å‹
    try:
        model = EvoPointDALitModule.load_from_checkpoint(ckpt_path)
        model.eval()
        if torch.cuda.is_available():
            model.cuda()
    except Exception as e:
        print(f"âŒ Load Error: {e}")
        return None
    
    # 2. åŠ è½½æ•°æ®
    try:
        dataset = SimpleFolderDataset(data_folder)
        loader = DataLoader(dataset, batch_size=1, shuffle=False)
    except Exception as e:
        print(f"âŒ Data Error: {e}")
        return None
    
    all_labels = []
    all_probs = []
    
    # 3. æ¨ç†
    with torch.no_grad():
        for batch in tqdm(loader, desc="Inference", leave=False):
            if torch.cuda.is_available():
                batch = batch.cuda()
            
            src_x = batch.x if model.hparams.use_esm else None
            feats, _ = model.backbone(src_x, batch.pos, batch.batch)
            logits = model.seg_head(feats)
            probs = torch.softmax(logits, dim=1)[:, 1]
            
            # è¿™é‡Œçš„ç½®ä¿¡åº¦è¿‡æ»¤ä¿æŒä¸æ¨¡å‹è®­ç»ƒæ—¶çš„é…ç½®ä¸€è‡´
            if hasattr(model, '_normalize_plddt'):
                p = model._normalize_plddt(batch.plddt).squeeze()
            else:
                p = batch.plddt.squeeze() / 100.0
            
            # [ä¿®æ”¹] æ³¨é‡Šæ‰ç¡¬æˆªæ–­é€»è¾‘ï¼Œä¿ç•™åŸå§‹æ¦‚ç‡
            # å¦‚æœæ¨¡å‹ä½¿ç”¨äº† plddt æƒé‡ï¼Œåˆ™åº”ç”¨è¿‡æ»¤é€»è¾‘
            # if model.hparams.use_plddt_weight:
            #     is_reliable = (p > 0.65).float()
            #     probs = probs * is_reliable
            
            all_labels.append(batch.y.cpu().numpy())
            all_probs.append(probs.cpu().numpy())

    y_true = np.concatenate(all_labels)
    y_scores = np.concatenate(all_probs)
    
    # 4. å¯»æ‰¾æœ€ä½³ Standard IoU
    best_iou = 0.0
    best_thresh = 0.0
    
    # éå†é˜ˆå€¼
    for thresh in np.arange(0.1, 0.95, 0.05):
        y_pred = (y_scores > thresh).astype(int)
        
        # average='binary': ä»…è®¡ç®— Class 1 (Binding Site) çš„ IoU
        iou = jaccard_score(y_true, y_pred, average='binary')
        
        if iou > best_iou:
            best_iou = iou
            best_thresh = thresh

    print(f"   âœ… Best Standard IoU: {best_iou*100:.2f}% (Thresh={best_thresh:.2f})")
    
    return {
        "Experiment": exp_id,
        "Seed": seed,
        "Best_Standard_IoU": best_iou * 100,
        "Best_Threshold": best_thresh,
        "Checkpoint": ckpt_path
    }

def main():
    # 2. å…³é”®ä¿®å¤ï¼šå›ºå®šå…¨å±€éšæœºç§å­
    # è¿™ç¡®ä¿äº† PointNet++ ä¸­çš„é‡‡æ · (FPS) ä»¥åŠä»»ä½•å…¶ä»–éšæœºæ“ä½œæ¯æ¬¡éƒ½æ˜¯ç¡®å®šçš„
    pl.seed_everything(SEED, workers=True)
    print(f"ğŸ”’ Global seed set to {SEED}")

    tasks = get_checkpoints_from_dir(WEIGHTS_DIR)
    
    if not tasks:
        print("No checkpoints found. Please ensure 'saved_weights' exists.")
        return

    results = []
    
    for task in tasks:
        metrics = evaluate_standard_iou(task, AF2_DATA_FOLDER)
        if metrics:
            results.append(metrics)
            pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)

    if len(results) > 0:
        df = pd.DataFrame(results)
        
        print("\n" + "="*60)
        print("ğŸ† STANDARD IoU SUMMARY (Mean Â± Std)")
        print("="*60)
        
        if "Experiment" in df.columns:
            groups = df.groupby("Experiment")
            summary_rows = []
            
            print(f"{'Experiment':<25} | {'Std IoU (Best)':<20}")
            print("-" * 50)
            
            for name, group in groups:
                m_iou, s_iou = group["Best_Standard_IoU"].mean(), group["Best_Standard_IoU"].std()
                s_iou = 0.0 if np.isnan(s_iou) else s_iou
                
                iou_str = f"{m_iou:.2f} Â± {s_iou:.2f}"
                
                print(f"{name:<25} | {iou_str:<20}")
                
                summary_rows.append({
                    "Experiment": f"{name} (Mean)",
                    "Seed": "Aggregated",
                    "Best_Standard_IoU": iou_str,
                    "Best_Threshold": "-",
                    "Checkpoint": "-"
                })
            
            df_final = pd.concat([df, pd.DataFrame(summary_rows)], ignore_index=True)
            df_final.to_csv(OUTPUT_CSV, index=False)
            
        print(f"\nğŸ“ Results saved to: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
