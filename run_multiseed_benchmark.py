import subprocess
import os
import shutil
import numpy as np
import pandas as pd
from collections import defaultdict
import glob  # æ–°å¢: ç”¨äºæŸ¥æ‰¾æ–‡ä»¶

SEEDS = [42, 2025, 1]
MAX_EPOCHS = 100

EXPERIMENTS = [
    {
        "id": "A",
        "name": "Baseline (Pure Geom)",
        "args": "model.use_esm=False model.use_da=False model.feature_dim=128"
    },
    {
        "id": "B",
        "name": "Ours (No DA)",
        "args": "model.use_esm=True model.use_da=False"
    },
    {
        "id": "C",
        "name": "Ours (Standard DANN)",
        "args": "model.use_esm=True model.use_da=True model.use_plddt_weight=False"
    },
    {
        "id": "D",
        "name": "Ours (Confidence-Aware)",
        "args": "model.use_esm=True model.use_da=True model.use_plddt_weight=True"
    }
]

def clean_checkpoints():
    """è®­ç»ƒå‰æ¸…ç©ºä¸´æ—¶ç›®å½•ï¼Œé˜²æ­¢æ··æ·†"""
    if os.path.exists("checkpoints"):
        try:
            shutil.rmtree("checkpoints")
        except Exception as e:
            print(f"âš ï¸ Warning: Failed to clean checkpoints: {e}")
    os.makedirs("checkpoints", exist_ok=True)

def backup_checkpoints(exp_id, seed):
    """
    [æ–°å¢åŠŸèƒ½] å°†è®­ç»ƒå¥½çš„æƒé‡å¤‡ä»½åˆ° saved_weights/ ç›®å½•
    ç»“æ„: saved_weights/A_42/best-epoch=xx.ckpt
    """
    backup_dir = os.path.join("saved_weights", f"{exp_id}_{seed}")
    os.makedirs(backup_dir, exist_ok=True)
    
    # æŸ¥æ‰¾ checkpoints ç›®å½•ä¸‹çš„æ‰€æœ‰ .ckpt æ–‡ä»¶
    found = False
    for ckpt in glob.glob(os.path.join("checkpoints", "*.ckpt")):
        shutil.copy(ckpt, backup_dir)
        print(f"ğŸ“¦ Backup: {ckpt} -> {backup_dir}/")
        found = True
    
    if not found:
        print(f"âš ï¸ Warning: No checkpoints found to backup for Exp {exp_id} Seed {seed}")

def run_command(cmd, log_file):
    print(f"ğŸ‘‰ Exec: {cmd}")
    output_buffer = ""
    try:
        with open(log_file, "w") as f:
            process = subprocess.Popen(
                cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
            )
            for line in process.stdout:
                print(line, end="")
                f.write(line)
                output_buffer += line
            process.wait()
            if process.returncode != 0:
                print(f"âŒ Command failed with return code {process.returncode}")
                # æ‰“å°æœ€åå‡ è¡Œé”™è¯¯æ—¥å¿—
                print("   [Last 5 lines of log]")
                print("   " + "\n   ".join(output_buffer.splitlines()[-5:]))
    except Exception as e:
        print(f"âŒ Execution Error: {e}")
    return output_buffer

def parse_metrics(output):
    metrics = {"Overall_IoU": 0.0, "Best_Threshold": 0.0}
    if "<<<METRICS_START>>>" in output:
        try:
            block = output.split("<<<METRICS_START>>>")[1].split("<<<METRICS_END>>>")[0]
            for line in block.strip().split("\n"):
                if ":" in line:
                    k, v = line.split(":")
                    key = k.strip()
                    val_str = v.strip().replace('%', '')
                    try:
                        metrics[key] = float(val_str)
                    except:
                        pass
        except Exception as e:
            print(f"âš ï¸ Error parsing metrics: {e}")
    return metrics

def main():
    results = defaultdict(list)
    print("="*60)
    print(f"ğŸ§¬ EvoPoint-DA Multi-Seed Benchmark (100 Epochs, GPU Enabled)")
    print("="*60)

    total_runs = len(SEEDS) * len(EXPERIMENTS)
    current_run = 0

    for seed in SEEDS:
        print(f"\nğŸŒ± === Starting Loop for SEED {seed} ===\n")
        
        for exp in EXPERIMENTS:
            current_run += 1
            exp_id = exp['id']
            print(f"[{current_run}/{total_runs}] Running Experiment {exp_id} (Seed {seed})...")
            
            # 1. æ¸…ç†ä¸´æ—¶æƒé‡ç›®å½• (å‡†å¤‡å¼€å§‹æ–°è®­ç»ƒ)
            clean_checkpoints()
            
            log_train = f"logs/benchmark/train_{exp_id}_seed_{seed}.txt"
            os.makedirs(os.path.dirname(log_train), exist_ok=True)
            
            # 2. è®­ç»ƒ
            train_cmd = " ".join([
                "python train.py",
                exp['args'],
                f"+seed_everything={seed}",
                f"trainer.max_epochs={MAX_EPOCHS}",
                f"+trainer.default_root_dir=logs/benchmark/{exp_id}_{seed}",
                "trainer.accelerator=cpu" # å¦‚æœæœ‰GPUè¯·æ”¹ä¸º gpu
            ])
            run_command(train_cmd, log_train)
            
            # 3. æµ‹è¯•
            log_test = f"logs/benchmark/test_{exp_id}_seed_{seed}.txt"
            test_cmd = "python test_adaptive.py" 
            test_output = run_command(test_cmd, log_test)
            
            # 4. [ä¿®æ”¹ç‚¹] å¤‡ä»½æƒé‡ï¼
            # å¿…é¡»åœ¨ clean_checkpoints ä¹‹å‰ (å³ä¸‹ä¸€æ¬¡å¾ªç¯å¼€å§‹å‰) æ‰§è¡Œ
            backup_checkpoints(exp_id, seed)

            # 5. è®°å½•ç»“æœ
            m = parse_metrics(test_output)
            results[exp_id].append(m)
            print(f"âœ… Exp {exp_id} (Seed {seed}) Result: IoU={m['Overall_IoU']}% (Thresh={m['Best_Threshold']})")

    # === Report ===
    print("\n\n" + "="*80)
    print("ğŸ† FINAL BENCHMARK REPORT (100 Epochs)")
    print("="*80)

    def fmt_stat(exp_id, key):
        vals = [r[key] for r in results[exp_id]]
        if not vals: return "N/A"
        return f"{np.mean(vals):.2f} Â± {np.std(vals):.2f}"

    print("\n### Table 2: Ablation Study")
    print("| ID | Model | IoU (%) | Best Threshold |")
    print("|---|---|---|---|")
    
    for exp in EXPERIMENTS:
        iou_str = fmt_stat(exp['id'], 'Overall_IoU')
        thresh_str = fmt_stat(exp['id'], 'Best_Threshold')
        print(f"| {exp['id']} | {exp['name']} | **{iou_str}** | {thresh_str} |")

    print("\nâœ… Done. All weights saved to 'saved_weights/' directory.")

if __name__ == "__main__":
    main()
