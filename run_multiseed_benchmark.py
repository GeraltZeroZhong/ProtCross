import subprocess
import os
import shutil
import numpy as np
import pandas as pd
from collections import defaultdict

# === é…ç½®åŒºåŸŸ ===
SEEDS = [42, 1234, 2024]
MAX_EPOCHS = 50

# å®šä¹‰å››ä¸ªå…³é”®å®éªŒ (ä¸ run_benchmark.py ä¸€è‡´)
EXPERIMENTS = [
    {
        "id": "A",
        "name": "Baseline (Pure Geom)",
        "args": "model.use_esm=False model.use_da=False model.feature_dim=128"
    },
    {
        "id": "B",
        "name": "Ours (No DA)",
        "args": "model.use_esm=True model.use_da=False"
    },
    {
        "id": "C",
        "name": "Ours (Standard DANN)",
        "args": "model.use_esm=True model.use_da=True model.use_plddt_weight=False"
    },
    {
        "id": "D",
        "name": "Ours (Confidence-Aware)",
        "args": "model.use_esm=True model.use_da=True model.use_plddt_weight=True"
    }
]

def clean_checkpoints():
    """æ¸…ç† checkpoints æ–‡ä»¶å¤¹ï¼Œé˜²æ­¢ test è¯»å–åˆ°æ—§çš„æƒé‡"""
    if os.path.exists("checkpoints"):
        try:
            shutil.rmtree("checkpoints")
        except Exception as e:
            print(f"âš ï¸ Warning: Failed to clean checkpoints: {e}")
    os.makedirs("checkpoints", exist_ok=True)

def run_command(cmd, log_file):
    """è¿è¡Œå‘½ä»¤å¹¶å°†è¾“å‡ºåŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ§åˆ¶å° (ç®€åŒ–ç‰ˆè¾“å‡º)"""
    print(f"ğŸ‘‰ Exec: {cmd}")
    output_buffer = ""
    try:
        with open(log_file, "w") as f:
            process = subprocess.Popen(
                cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
            )
            for line in process.stdout:
                f.write(line)
                output_buffer += line
            process.wait()
            if process.returncode != 0:
                print(f"âŒ Command failed with return code {process.returncode}")
    except Exception as e:
        print(f"âŒ Execution Error: {e}")
    return output_buffer

def parse_metrics(output):
    """ä» test.py çš„è¾“å‡ºä¸­æå–æŒ‡æ ‡"""
    metrics = {
        "Overall_IoU": 0.0, 
        "High_IoU": 0.0, 
        "Med_IoU": 0.0, 
        "Low_FPR": 0.0
    }
    
    # æŸ¥æ‰¾ <<<METRICS_START>>> å— (å…¼å®¹ä½ çš„ test.py è¾“å‡ºæ ¼å¼)
    if "<<<METRICS_START>>>" in output:
        try:
            block = output.split("<<<METRICS_START>>>")[1].split("<<<METRICS_END>>>")[0]
            for line in block.strip().split("\n"):
                if ":" in line:
                    k, v = line.split(":")
                    key = k.strip()
                    val = float(v.strip().replace('%', '')) # ç§»é™¤%å¹¶è½¬float
                    if key in metrics:
                        metrics[key] = val
        except Exception as e:
            print(f"âš ï¸ Error parsing metrics: {e}")
            
    return metrics

def main():
    # å­˜å‚¨ç»“æ„: results[exp_id] = list of dicts (one per seed)
    results = defaultdict(list)
    
    print("="*60)
    print(f"ğŸ§¬ EvoPoint-DA Multi-Seed Benchmark (Seeds: {SEEDS})")
    print("="*60)

    total_runs = len(SEEDS) * len(EXPERIMENTS)
    current_run = 0

    for seed in SEEDS:
        print(f"\nğŸŒ± === Starting Loop for SEED {seed} ===\n")
        
        for exp in EXPERIMENTS:
            current_run += 1
            exp_id = exp['id']
            print(f"[{current_run}/{total_runs}] Running Experiment {exp_id} (Seed {seed})...")
            
            # 1. æ¸…ç†æƒé‡
            clean_checkpoints()
            
            # 2. è®­ç»ƒ (åŠ å…¥ +seed_everything)
            log_train = f"logs/benchmark/train_{exp_id}_seed_{seed}.txt"
            os.makedirs(os.path.dirname(log_train), exist_ok=True)
            
            train_cmd = (
                f"python train.py {exp['args']} "
                f"+seed_everything={seed} "
                f"trainer.max_epochs={MAX_EPOCHS} "
                f"trainer.default_root_dir=logs/benchmark/{exp_id}_{seed}"
            )
            run_command(train_cmd, log_train)
            
            # 3. æµ‹è¯•
            log_test = f"logs/benchmark/test_{exp_id}_seed_{seed}.txt"
            test_cmd = "python test.py" # test.py ä¼šè‡ªåŠ¨æ‰¾ checkpoints é‡Œæœ€æ–°çš„
            test_output = run_command(test_cmd, log_test)
            
            # 4. è®°å½•æ•°æ®
            m = parse_metrics(test_output)
            results[exp_id].append(m)
            print(f"âœ… Exp {exp_id} (Seed {seed}) Result: IoU={m['Overall_IoU']}%")

    # ==========================================
    # ğŸ“Š FINAL REPORT GENERATION
    # ==========================================
    print("\n\n" + "="*80)
    print("ğŸ† FINAL MULTI-SEED BENCHMARK REPORT")
    print("="*80)

    # --- Helper to format Mean Â± Std ---
    def fmt_stat(exp_id, metric_key):
        vals = [r[metric_key] for r in results[exp_id]]
        if not vals: return "N/A"
        mean = np.mean(vals)
        std = np.std(vals)
        return f"{mean:.2f} Â± {std:.2f}"

    # --- Table 2: Ablation Study ---
    print("\n### Table 2: Ablation Study (Mean Â± Std over 3 runs)")
    print("| ID | Model | ESM | DA | pLDDT | AF2 IoU (%) |")
    print("|---|---|---|---|---|---|")
    
    for exp in EXPERIMENTS:
        args = exp['args']
        esm = "âŒ" if "use_esm=False" in args else "âœ…"
        da = "âŒ" if "use_da=False" in args else "âœ…"
        plddt = "âŒ" if "use_plddt_weight=False" in args or "use_da=False" in args else "âœ…"
        
        iou_str = fmt_stat(exp['id'], 'Overall_IoU')
        print(f"| {exp['id']} | {exp['name']} | {esm} | {da} | {plddt} | **{iou_str}** |")

    # --- Table 3: Confidence Analysis (Experiment D only) ---
    print("\n### Table 3: Analysis by Confidence (Experiment D, Mean Â± Std)")
    print("| Region Type | Metric | Value |")
    print("|---|---|---|")
    
    if 'D' in results and results['D']:
        high = fmt_stat('D', 'High_IoU')
        med = fmt_stat('D', 'Med_IoU')
        low_fpr = fmt_stat('D', 'Low_FPR')
        
        print(f"| High Conf (>90) | IoU | {high} |")
        print(f"| Med Conf (70-90) | IoU | {med} |")
        print(f"| Low Conf (<70) | False Positive Rate | **{low_fpr}** |")
    else:
        print("| Experiment D data missing | - | - |")

    print("\nâœ… Benchmark Completed. Logs are saved in 'logs/benchmark/'.")

if __name__ == "__main__":
    main()